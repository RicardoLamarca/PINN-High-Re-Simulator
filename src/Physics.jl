# src/physics.jl
using NeuralPDE, Lux, ModelingToolkit, Optimization, OptimizationOptimJL, CSV, DataFrames, ComponentArrays

"""
    train_pinn(data_folder; max_iters=1000)

Trains a Physics-Informed Neural Network (PINN) using both the Navier-Stokes 
residuals and the CSV data generated by the simulator.
"""
function train_pinn(data_folder; max_iters=1000)
    # 1. Load simulation data for the "Additional Loss"
    # We fetch the CSV files generated by the simulator to guide the network
    files = filter(f -> endswith(f, ".csv"), readdir(data_folder))
    if isempty(files)
        error("No CSV files found in $data_folder. Please run the simulator first.")
    end
    
    # Read the first available simulation (e.g., Re=1e6)
    df = CSV.read(joinpath(data_folder, files[1]), DataFrame)
    
    # Downsample points to speed up the training process
    df_sub = df[1:5:end, :] 

    # 2. Define Domain and Variables
    # We define independent variables (x, y) and dependent variables (u, v, p)
    @parameters x y
    @variables u(..) v(..) p(..)
    Dx = Differential(x); Dy = Differential(y)
    Dxx = Differential(x)^2; Dyy = Differential(y)^2

    # Physical parameters (Must match the simulator's settings)
    Re = 1e6
    nu = 1.0 / Re

    # 3. Navier-Stokes Equations (Physics Loss)
    # These represent the physical residuals that the AI must minimize
    eqs = [
        # Continuity equation: div(U) = 0
        Dx(u(x, y)) + Dy(v(x, y)) ~ 0, 
        
        # Momentum X: u*ux + v*uy = -px + nu*(uxx + uyy)
        u(x, y)*Dx(u(x, y)) + v(x, y)*Dy(u(x, y)) ~ -Dx(p(x, y)) + nu*(Dxx(u(x, y)) + Dyy(u(x, y))), 
        
        # Momentum Y: u*vx + v*vy = -py + nu*(vxx + vyy)
        u(x, y)*Dx(v(x, y)) + v(x, y)*Dy(v(x, y)) ~ -Dy(p(x, y)) + nu*(Dxx(v(x, y)) + Dyy(v(x, y)))
    ]

    # 4. Boundary Conditions and Obstacle Geometry
    # Defining the cylinder obstacle as a penalty or boundary condition
    cx, cy, r = 0.5, 0.5, 0.1
    
    bcs = [
        u(0, y) ~ 1.0,  # Dirichlet Inlet: Constant flow
        v(0, y) ~ 0.0,
        u(x, 0) ~ 0.0,  # No-slip Walls (Bottom)
        u(x, 1) ~ 0.0,  # No-slip Walls (Top)
        p(2, y) ~ 0.0   # Pressure Outlet: Zero relative pressure
    ]

    # 5. Neural Network Architecture
    # Input: (x, y) -> Output: (u, v, p)
    input_dims = 2
    hidden_units = 20
    chain = Lux.Chain(
        Lux.Dense(input_dims, hidden_units, Lux.tanh),
        Lux.Dense(hidden_units, hidden_units, Lux.tanh),
        Lux.Dense(hidden_units, 3) # Output layer for u, v, and p
    )

    # 6. Data-Driven Loss (Additional Loss)
    # This function penalizes the difference between NN predictions and simulator data
    function additional_loss(phi, res, x_data, y_data, u_data)
        loss = 0.0
        for i in 1:size(df_sub, 1)
            # Get prediction from the neural network at specific coordinates
            pred = phi([df_sub.x[i], df_sub.y[i]], res)
            
            # Mean Squared Error against CSV values
            loss += abs2(pred[1] - df_sub.u[i]) + abs2(pred[2] - df_sub.v[i])
        end
        return loss / size(df_sub, 1)
    end

    # 7. Discretization and Optimization Strategy
    # Using a grid-based strategy for collocation points
    strategy = NeuralPDE.GridTraining(0.05)
    discretization = PhysicsInformedNN(chain, strategy, additional_loss=additional_loss)

    # Compile the PDE system
    domains = [x ∈ [0.0, 2.0], y ∈ [0.0, 1.0]]
    @named pde_system = PDESystem(eqs, bcs, domains, [x, y], [u(x, y), v(x, y), p(x, y)])
    
    # Convert the PDE to an Optimization Problem
    prob = discretize(pde_system, discretization)

    # 8. Training with L-BFGS
    # L-BFGS is generally more efficient than Adam for physical constraints
    println("Starting PINN training...")
    res = Optimization.solve(prob, BFGS(), maxiters=max_iters)
    
    return res, discretization
end
